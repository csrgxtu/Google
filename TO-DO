28/Feb/2014
get myself familiar with apache's httpclient, and implement a class
that is responsible to download a url link.

12/Mar/2014
well, it is wired, i remember that last time i tested the mongodb
in this project, and synched with the github, but this time, when
i check, there is no such thing. well, whatever, I need to create
database and store the visited and unvisited table in the mongo
database.

14/Mar/2014
focus on how to solve the abs urls problems in RetrieveHref.java

15/Mar/2014
focus on solving the variaty url problems, i.e, i want url is sth
standard like "http://blogxtu.zapto.org/", if the url isnt a url
or is empty or point to a big large file, it needs to be filted.
also, I want to  implement a functionality to crawl only a website,
just like what Nutch does.

17/Mar/2014
Well, the ShadowWalker can work, but it didn't perfect, eat up 40
percent of the CPU and 20 percent Memory. and sometimes stuck with
an URL, long time without responding, it seems that the set
connection timeout didnt do the work, well, httpclient is such a
basterd, i am gonna to use hard way to limit the connection time,
and write a domain filter to restrict the url into only a domain.

18/Mar/2014
In the formal post, i declare to fix the efficiency problem, but
after searching and testing, i found the the http client set
connection time out is working right, only sometimes it is too
heavy to establish connection with MongoDB, and there are some
methods didn't return a value, that is not me, gonna change this
from time to time. and this time, i am gonna to write a FileSaver
class that save a string to a file in the disk.

18/Mar/2014
Now, FileSaver is working, its time to write a whole class that
download all the pages from a target domain and save the file
into disk. oh, by the way, i need to filter the url that is used
for section location in a html page, ie 
  http://blogxtu.zapto.org/index.html#readme
  
19/Mar/2014
HtmlDownloader is working fine, LoadUrlFromDB is fine, SaveUrlToDB
is fine, UrlFilter is fine, DomainFilter is fine, RetrieveHref is
fine, FileSaver is fine too, lets write a Main program that can
make it all work together. oh, you need write a class that is
responsible for read seed and init the database in Mongo.

19/Mar/2014
write a main program that make all class work together!!!

20/Mar/2014
the Main program that crawls a specified a virtualhost works very
well, but there still something I need to do, first, use
configuration file in program, dont hard code, second, sevral urls
will point to the same page, so it means that u will dumplicate
the html file, third, the DomainFilter is not that good.

22/Mar/2014
implement the search page!!!!

25/Mar/2014
well, the crawler can work, but it is too slow, and establishes too
much connection to the MongoDB server, this is too heavy. and also
it the crawler dosent support distributed crawling. here, i am gona
to implement UrlDBManager class that schedules the operation on
visited and unvisited table, at the same time redisign the way
client program communicate with the MongoDB. and then, i will 
standarlize the project using package manager of Java. thats enough
for today. oh, after a searching around, i found that MongoDB dont
support transactions as MySQL does, so without transactions, I cant
gurateen the efficiency of the distributed crawling.
